{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> \n",
    "Case Study: MOMENT for ECG Classification using PTB-XL, a large publicly available electrocardiography dataset\n",
    "</h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "### 1. PTB-XL dataset\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.1 Download PTB-XL dataset\n",
    "### 2. Loading MOMENT\n",
    "### 3. Method 1: Learning a Statistical ML Classifier on MOMENT Embeddings\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Load PTB-XL dataset\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Dignostic label Classification using raw ECG signal\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Dignostic label Classification using MOMENT embedding on ECG signal\n",
    "### 4. Method 2: Finetuning Linear Classification Head\n",
    "### 5. Method 3: Full Finetuning MOMENT\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.1 Assess MOMENT embedding with SVM after finetuning the encoder\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5.2 Training with Multiple GPUs and Parameter Efficient FineTuning (PEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem: Classifying Abnormal ECG using PTB-XL\n",
    "\n",
    "[PTB-XL](https://www.nature.com/articles/s41597-020-0495-6) is a large publicly available electrocardiography (ECG) dataset. It comprises of 21,837 12-lead, 10 seconds long ECG recordings collected from 18,885 patients. The ECG-waveform data was annotated by two cardiologists as a multi-label dataset, where diagnostic labels were further aggregated into super and subclasses. In this notebook, we will classify each 10 second ECG recording into one of 5 SCP ECG classes: (1) Normal ECG, (2) Conduction Disturbance, (3) Myocardial Infarction, (4) Hypertrophy, and (5) ST/T change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download PTB-XL dataset\n",
    "\n",
    "PTB-XL is avaliable on Physionet, and can be downloaded [here](https://physionet.org/content/ptb-xl/1.0.3/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading MOMENT\n",
    "\n",
    "We will first install the MOMENT package, load some essential packages and the pre-trained model. \n",
    "\n",
    "MOMENT can be loaded in 4 modes: (1) `reconstruction`, (2) `embedding`, (3) `forecasting`, and (4) `classification`.\n",
    "\n",
    "In the `reconstruction` mode, MOMENT reconstructs input time series, potentially containing missing values. We can solve imputation and anomaly detection problems in this mode. This mode is suitable for solving imputation and anomaly detection tasks. During pre-training, MOMENT is trained to predict the missing values within uniformly randomly masked patches (disjoint sub-sequences) of the input time series, leveraging information from observed data in other patches. As a result, MOMENT comes equipped with a pre-trained reconstruction head, enabling it to address imputation and anomaly detection challenges in a zero-shot manner! Check out the `anomaly_detection.ipynb` and `imputation.ipynb` notebooks for more details!\n",
    "\n",
    "In the `embedding` model, MOMENT learns a $d$-dimensional embedding (e.g., $d=1024$ for `MOMENT-1-large`) for each input time series. These embeddings can be used for clustering and classification. MOMENT can learn embeddings in a zero-shot setting! Check out `classification.ipynb` notebook for more details! \n",
    "\n",
    "The `forecasting` and `classification` modes are used for forecasting and classification tasks, respectively. In these modes, MOMENT learns representations which are subsequently mapped to the forecast horizon or the number of classes, using linear forecasting and classification heads. Both the forecasting and classification head are randomly initialized, and therefore must be fine-tuned before use. Check out the `forecasting.ipynb` notebook for more details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy pandas scikit-learn matplotlib tqdm\n",
    "# !pip install git+https://github.com/moment-timeseries-foundation-model/moment.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from momentfm import MOMENTPipeline\n",
    "\n",
    "model = MOMENTPipeline.from_pretrained(\n",
    "    \"AutonLab/MOMENT-1-large\", \n",
    "    model_kwargs={\n",
    "        'task_name': 'classification',\n",
    "        'n_channels': 12, # number of input channels\n",
    "        'num_class': 5,\n",
    "        'freeze_encoder': True, # Freeze the patch embedding layer\n",
    "        'freeze_embedder': True, # Freeze the transformer encoder\n",
    "        'freeze_head': False, # The linear forecasting head must be trained\n",
    "        ## NOTE: Disable gradient checkpointing to supress the warning when linear probing the model as MOMENT encoder is frozen\n",
    "        'enable_gradient_checkpointing': False,\n",
    "        # Choose how embedding is obtained from the model: One of ['mean', 'concat']\n",
    "        # Multi-channel embeddings are obtained by either averaging or concatenating patch embeddings \n",
    "        # along the channel dimension. 'concat' results in embeddings of size (n_channels * d_model), \n",
    "        # while 'mean' results in embeddings of size (d_model)\n",
    "        'reduction': 'mean',\n",
    "    },\n",
    "    # local_files_only=True,  # Whether or not to only look at local files (i.e., do not try to download the model).\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOMENTPipeline(\n",
      "  (normalizer): RevIN()\n",
      "  (tokenizer): Patching()\n",
      "  (patch_embedding): PatchEmbedding(\n",
      "    (value_embedding): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    (position_embedding): PositionalEmbedding()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 16)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (head): ClassificationHead(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear): Linear(in_features=1024, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.init()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 341231104\n"
     ]
    }
   ],
   "source": [
    "# Number of parameters in the encoder\n",
    "num_params = sum(p.numel() for p in model.encoder.parameters())\n",
    "print(f\"Number of parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os \n",
    "import torch \n",
    "import numpy as np \n",
    "\n",
    "def control_randomness(seed: int = 42):\n",
    "    \"\"\"Function to control randomness in the code.\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "control_randomness(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Method 1: Learning a Statistical ML Classifier\n",
    "\n",
    "First we will use MOMENT to generate powerful representations of ECG time series in zero-shot settings. Note that the PTB-XL dataset was not observed during pre-training. \n",
    "\n",
    "We will use these representations and labels from the PTB-XL dataset to train a Support Vector Machine (SVM) classifier. To illustrate the value of MOMENT's representations, we will also train another SVM classifier with the raw ECG data. This setting is common in field of unsupervised representation learning, where the goal is to learn meaningful time series representations without any labeled data (see [TS2Vec](https://arxiv.org/pdf/2106.10466) for a recent example). The quality of these representations are evaluated based on the performance of the downstream classifier (in this case, SVM). This is also the setting that we consider in our [paper](https://arxiv.org/abs/2402.03885). \n",
    "\n",
    "Checkout `representation_learning.ipynb` for details on how we can use MOMENT to embed time series data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load PTBXL dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have downloaded the PTB-XL dataset, make sure to unzip it! The PTB-XL dataset will read and pre-process the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading PTB-XL\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'basepath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m args \u001b[38;5;241m=\u001b[39m Config()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#create dataloader for training and testing\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPTBXL_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m PTBXL_dataset(args, phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m PTBXL_dataset(args, phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/moment/momentfm/data/ptbxl_classification_dataset.py:168\u001b[0m, in \u001b[0;36mPTBXL_dataset.__init__\u001b[0;34m(self, config, phase)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeseries, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_texts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ecg_ids \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fp)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# Load path to the data and get dataframe with patient attributes, \u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# data path, labels and expert interpretations\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     paths_to_files \u001b[38;5;241m=\u001b[39m load_paths(\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasepath\u001b[49m, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs)\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m modify_df(config\u001b[38;5;241m.\u001b[39mbasepath, output_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_type)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Modified database\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Config' object has no attribute 'basepath'"
     ]
    }
   ],
   "source": [
    "from momentfm.data.ptbxl_classification_dataset import PTBXL_dataset\n",
    "import torch \n",
    "\n",
    "class Config:\n",
    "    # Path to the unzipped PTB-XL dataset folder\n",
    "    basepath = '/zfsauton/project/public/Mononito/ptb-xl' # 'path/to/ptbxl_dataset'\n",
    "\n",
    "    #path to cache directory to store preprocessed dataset if needed\n",
    "    #note that preprocessing the dataset is time consuming so you might be benefited to cache it\n",
    "    cache_dir = '/home/scratch/mgoswami/' # 'path/to/cache_dir'\n",
    "    load_cache = True\n",
    "\n",
    "    #sampling frequency, choose from 100 or 500\n",
    "    fs = 100\n",
    "\n",
    "    # Class to predict\n",
    "    code_of_interest = 'diagnostic_class'\n",
    "    output_type = 'Single'\n",
    "\n",
    "    #sequence length, only support 512 for now\n",
    "    seq_len = 512\n",
    "\n",
    "args = Config()\n",
    "\n",
    "#create dataloader for training and testing\n",
    "train_dataset = PTBXL_dataset(args, phase='train')\n",
    "test_dataset = PTBXL_dataset(args, phase='test')\n",
    "val_dataset = PTBXL_dataset(args, phase='val')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dignostic label Classification using raw ECG signal\n",
    "\n",
    "In this setting, we concat raw ECG signal along the channel dimension, and feed the concatenated time-series directly into a SVM. The goal is to provide a baseline to assess MOMENT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    "import numpy as np \n",
    "\n",
    "def get_timeseries(dataloader: DataLoader, agg='mean'):\n",
    "    '''\n",
    "    We provide two aggregation methods to convert the 12-lead ECG (2-dimensional) to a 1-dimensional time-series for SVM training:\n",
    "    - mean: average over all channels, result in [1 x seq_len] for each time-series\n",
    "    - channel: concat all channels, result in [1 x seq_len * num_channels] for each time-series\n",
    "\n",
    "    labels: [num_samples]\n",
    "    ts: [num_samples x seq_len] or [num_samples x seq_len * num_channels]\n",
    "\n",
    "    *note that concat all channels will result in a much larger feature dimensionality, thus making the fitting process much slower\n",
    "    '''\n",
    "    ts, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_labels in tqdm(dataloader, total=len(dataloader)):\n",
    "            # [batch_size x 12 x 512]\n",
    "            if agg == 'mean':\n",
    "                batch_x = batch_x.mean(dim=1)\n",
    "                ts.append(batch_x.detach().cpu().numpy())\n",
    "            elif agg == 'channel':\n",
    "                ts.append(batch_x.view(batch_x.size(0), -1).detach().cpu().numpy())\n",
    "            labels.append(batch_labels)        \n",
    "\n",
    "    ts, labels = np.concatenate(ts), np.concatenate(labels)\n",
    "    return ts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a SVM classifier on the concatenated raw ECG signals\n",
    "from momentfm.models.statistical_classifiers import fit_svm\n",
    "\n",
    "train_embeddings, train_labels = get_timeseries(train_loader, agg='mean')\n",
    "clf = fit_svm(features=train_embeddings, y=train_labels)\n",
    "train_accuracy = clf.score(train_embeddings, train_labels)\n",
    "\n",
    "test_embeddings, test_labels = get_timeseries(test_loader)\n",
    "test_accuracy = clf.score(test_embeddings, test_labels)\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dignostic label Classification using MOMENT embedding on ECG signal\n",
    "\n",
    "In this setting, we use MOMENT to embed time series data (see `representation_learning.ipynb`). Next, we train a Support Vector Machine (SVM) classifier using these embeddings as features and labels. This setting is common in field of unsupervised representation learning, where the goal is to learn meaningful time series representations without any labeled data (see [TS2Vec](https://arxiv.org/pdf/2106.10466) for a recent example). The quality of these representations are evaluated based on the performance of the downstream classifier (in this case, SVM). This is also the setting that we consider in our [paper](https://arxiv.org/abs/2402.03885). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    "import numpy as np \n",
    "from momentfm.models.statistical_classifiers import fit_svm\n",
    "\n",
    "def get_embeddings(model, device, reduction, dataloader: DataLoader):\n",
    "    '''\n",
    "    labels: [num_samples]\n",
    "    embeddings: [num_samples x d_model]\n",
    "    '''\n",
    "    embeddings, labels = [], []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_labels in tqdm(dataloader, total=len(dataloader)):\n",
    "            # [batch_size x 12 x 512]\n",
    "            batch_x = batch_x.to(device).float()\n",
    "            # [batch_size x num_patches x d_model (=1024)]\n",
    "            output = model(x_enc=batch_x, reduction=reduction) \n",
    "            #mean over patches dimension, [batch_size x d_model]\n",
    "            embedding = output.embeddings.mean(dim=1)\n",
    "            embeddings.append(embedding.detach().cpu().numpy())\n",
    "            labels.append(batch_labels)        \n",
    "\n",
    "    embeddings, labels = np.concatenate(embeddings), np.concatenate(labels)\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set device to be 'cuda:0' or 'cuda' if you only have one GPU\n",
    "device = 'cuda:6'\n",
    "reduction = 'mean'\n",
    "train_embeddings, train_labels = get_embeddings(model, device, reduction, train_loader)\n",
    "clf = fit_svm(features=train_embeddings, y=train_labels)\n",
    "train_accuracy = clf.score(train_embeddings, train_labels)\n",
    "\n",
    "test_embeddings, test_labels = get_embeddings(model, device, reduction, test_loader)\n",
    "test_accuracy = clf.score(test_embeddings, test_labels)\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that MOMENT-extracted embedding improves test time accuracy from 60% to 76%! Note that PTB-XL ECG signals does NOT appear in MOMENT pretraining data. This performance improvement shows MOMENT's high quality representation generation ability under zero shot setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Finetuning the Linear Classification Head only\n",
    "\n",
    "In this setting, we freeze the MOMENT encoder and finetune the linear classification head using Cross Entropy Loss. MOMENT encoder is frozen by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, device, train_dataloader, criterion, optimizer, scheduler, reduction='mean'):\n",
    "    '''\n",
    "    Train only classification head\n",
    "    '''\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch_x, batch_labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        batch_x = batch_x.to(device).float()\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        #note that since MOMENT encoder is based on T5, it might experiences numerical unstable issue with float16\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float32):\n",
    "            output = model(x_enc=batch_x, reduction=reduction)\n",
    "            loss = criterion(output.logits, batch_labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    avg_loss = np.mean(losses)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(dataloader, model, criterion, device, phase='val', reduction='mean'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    total_loss, total_correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_labels in dataloader:\n",
    "            batch_x = batch_x.to(device).float()\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            output = model(x_enc=batch_x, reduction=reduction)\n",
    "            loss = criterion(output.logits, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (output.logits.argmax(dim=1) == batch_labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "\n",
    "epoch = 5\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.head.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=epoch * len(train_loader))\n",
    "device = 'cuda:3'\n",
    "\n",
    "for i in tqdm(range(epoch)):\n",
    "    train_loss = train_epoch(model, device, train_loader, criterion, optimizer, scheduler)\n",
    "    val_loss, val_accuracy = evaluate_epoch(val_loader, model, criterion, device, phase='test')\n",
    "    print(f'Epoch {i}, train loss: {train_loss}, val loss: {val_loss}, val accuracy: {val_accuracy}')\n",
    "\n",
    "test_loss, test_accuracy = evaluate_epoch(test_loader, model, criterion, device, phase='test')\n",
    "print(f'Test loss: {test_loss}, test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method 3: Full Finetuning MOMENT\n",
    "\n",
    "In this section, we unfreeze MOMENT encoder and finetune the full model on PTB-XL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading MOMENT with encoder unfrozen\n",
    "from momentfm import MOMENTPipeline\n",
    "\n",
    "model = MOMENTPipeline.from_pretrained(\n",
    "                                        \"AutonLab/MOMENT-1-large\", \n",
    "                                        model_kwargs={\n",
    "                                            'task_name': 'classification',\n",
    "                                            'n_channels': 12,\n",
    "                                            'num_class': 5,\n",
    "                                            'freeze_encoder': False,\n",
    "                                            'freeze_embedder': False,\n",
    "                                            'reduction': 'mean',\n",
    "                                        },\n",
    "                                        )\n",
    "model.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the learning rate should be smaller to guide the encoder to learn the task without forgetting the pre-trained knowledge\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "epoch = 5\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-4, total_steps=epoch * len(train_loader))\n",
    "device = 'cuda:3'\n",
    "\n",
    "for i in tqdm(range(epoch)):\n",
    "    train_loss = train_epoch(model, device, train_loader, criterion, optimizer, scheduler)\n",
    "    val_loss, val_accuracy = evaluate_epoch(val_loader, model, criterion, device, phase='test')\n",
    "    print(f'Epoch {i}, train loss: {train_loss}, val loss: {val_loss}, val accuracy: {val_accuracy}')\n",
    "\n",
    "test_loss, test_accuracy = evaluate_epoch(test_loader, model, criterion, device, phase='test')\n",
    "print(f'Test loss: {test_loss}, test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Assess MOMENT embedding with SVM after finetuning the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set device to be 'cuda:0' or 'cuda' if you only have one GPU\n",
    "device = 'cuda:3'\n",
    "reduction = 'mean'\n",
    "train_embeddings, train_labels = get_embeddings(model, device, reduction, train_loader)\n",
    "clf = fit_svm(features=train_embeddings, y=train_labels)\n",
    "train_accuracy = clf.score(train_embeddings, train_labels)\n",
    "\n",
    "test_embeddings, test_labels = get_embeddings(model, device, reduction, test_loader)\n",
    "test_accuracy = clf.score(test_embeddings, test_labels)\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that after MOMENT encoder is finetuned for downstream dataset, the embedding gives better test accuracy with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training with Multi-GPU and Parameter Efficient FineTuning (PEFT)\n",
    "\n",
    "It might be of interest to the research community with an example to train MOMENT with multi-gpu and PEFT approaches. We also offer a script where this could be achieved.\n",
    "\n",
    "Note that number of processes should be adjusted in the config file at in finetune_demo/ds.ymal according to your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3,4 accelerate launch --config_file tutorials/finetune_demo/ds.yaml \\\n",
    "    tutorials/finetune_demo/classification.py \\\n",
    "    --base_path path to your ptbxl base folder \\\n",
    "    --cache_dir path to cache directory for preprocessed dataset \\\n",
    "    --mode full_finetuning \\\n",
    "    --output_path path to store train log and checkpoint \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code also supports [LoRA](https://arxiv.org/abs/2106.09685) as a way of doing parameter efficient finetuning. To use LoRA, simply add a flag to the command line above. Currently, LoRA doesn't work well with deepspeed zero3, therefore one might consider switching to stage 2 for LoRA in finetune_demo/ds.ymal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=3,4 accelerate launch --config_file tutorials/finetune_demo/ds.yaml \\\n",
    "    tutorials/finetune_demo/classification.py \\\n",
    "    --base_path path to your ptbxl base folder \\\n",
    "    --cache_dir path to cache directory for preprocessed dataset \\\n",
    "    --mode full_finetuning \\\n",
    "    --output_path path to store train log and checkpoint \\\n",
    "    --lora"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
